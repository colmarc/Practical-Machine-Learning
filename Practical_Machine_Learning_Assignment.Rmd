---
title: "Practical ML Assignment"
author: "Colmarc"
date: "26 dicembre 2015"
output: html_document
---

Import necessary libraries
```{r}
library(caret) #ML library
library(rattle) #Visualization library
```

Set a specific seed to make the analysis 
```{r}
set.seed(11111)
```

Download the date from the sources, handling NA, DIV/0 and '' values
```{r}
Url_train <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
Url_test <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url(Url_train), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(Url_test), na.strings=c("NA","#DIV/0!",""))
```

Create data partition setting p=0.7
```{r}
inTrain <- createDataPartition(y=training$classe, p=0.7, list=FALSE)
myTraining <- training[inTrain, ]; myTesting <- training[-inTrain, ]
```

Analyze variables deleting the ones with low variance (zero or near zero)
```{r}
low_var_variable <- nearZeroVar(myTraining, saveMetrics=TRUE)
low_var_variables <- names(myTraining) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
"kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
"max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
"var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
"stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
"kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
"max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
"skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
"amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
"skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
"max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
"amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
"avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
"stddev_yaw_forearm", "var_yaw_forearm")
myTraining <- myTraining[!low_var_variables]
myTraining <- myTraining[c(-1)]
```

Delete variables with too many na values (>0.6)
```{r}
training_loop <- myTraining #creating another subset to iterate in loop
for(i in 1:length(myTraining)) { #for every column in the training dataset
        if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .6 ) { #if n?? NAs > 60% of total observations
        for(j in 1:length(training_loop)) {
            if( length( grep(names(myTraining[i]), names(training_loop)[j]) ) ==1)  { #if the columns are the same:
                training_loop <- training_loop[ , -j] #Remove that column
            }   
        } 
    }
}

myTraining <- training_loop
```


Dataset fine tuning and format check
```{r}
columns_1 <- colnames(myTraining)
columns_2 <- colnames(myTraining[, -58]) #already with classe column removed
myTesting <- myTesting[columns_1]
testing <- testing[columns_2]

for (i in 1:length(testing) ) {
        for(j in 1:length(myTraining)) {
        if( length( grep(names(myTraining[i]), names(testing)[j]) ) ==1)  {
            class(testing[j]) <- class(myTraining[i])
        }      
    }      
}
```


Model Fitting - Decision Tree (caret package)
```{r}
tree <- train(classe ~ ., data=myTraining, method="rpart")
fancyRpartPlot(tree$finalModel)
```

Model Prediction - Decision Tree
```{r}
predicted <- predict(tree, myTesting)
```

Model Analysis (confusion matrix) - Decision Tree
```{r}
confusionMatrix(predicted, myTesting$classe)
```


Model Fitting - Random Forest (rpat package)
```{r}
forest <- randomForest(classe ~. , data=myTraining)
```

Model Prediction - Random Forest
```{r}
predicted <- predict(forest, myTesting, type = "class")
```

Model Analysis (confusion matrix) - Random Forest
```{r}
confusionMatrix(predicted, myTesting$classe)
```

Results (Cross Validation and Error Estimation show a very good fitting of the model to the data. We can expect that the also the out of sample error remains low)
```{r}
          Reference
Prediction    A    B    C    D    E
         A 2231    2    0    0    0
         B    1 1516    2    0    0
         C    0    0 1366    3    0
         D    0    0    0 1282    2
         E    0    0    0    1 1440

Overall Statistics
                                          
               Accuracy : 0.9986          
                 95% CI : (0.9975, 0.9993)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9982          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9996   0.9987   0.9985   0.9969   0.9986
Specificity            0.9996   0.9995   0.9995   0.9997   0.9998
Pos Pred Value         0.9991   0.9980   0.9978   0.9984   0.9993
Neg Pred Value         0.9998   0.9997   0.9997   0.9994   0.9997
Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
Detection Rate         0.2843   0.1932   0.1741   0.1634   0.1835
Detection Prevalence   0.2846   0.1936   0.1745   0.1637   0.1837
Balanced Accuracy      0.9996   0.9991   0.9990   0.9983   0.9992
```